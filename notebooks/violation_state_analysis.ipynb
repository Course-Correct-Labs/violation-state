{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "# Violation State Analysis Notebook\n",
    "\n",
    "This notebook provides an interactive analysis of the Violation State study data.\n",
    "\n",
    "## Study Overview\n",
    "\n",
    "We investigate whether a copyright-related refusal at the start of a conversation leads to persistent refusals of subsequent, benign image generation requests in ChatGPT Web.\n",
    "\n",
    "**Hypothesis**: Copyright trigger → safety state contamination → elevated refusal rate\n",
    "\n",
    "## Final Results Summary\n",
    "\n",
    "**30 contaminated sessions** (120 image prompts total):\n",
    "- 116 refusals (96.67%)\n",
    "- 4 successes (3.33%)\n",
    "- Breakthroughs in threads: 12, 16, 25, 27\n",
    "\n",
    "**10 control sessions** (40 image prompts total):\n",
    "- 0 refusals (0%)\n",
    "- 40 successes (100%)\n",
    "\n",
    "**Coffee cup paradox**: The coffee cup prompt was refused 100% of the time (30/30) in contaminated sessions, despite having zero semantic connection to copyright or real estate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2356dbdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies (for Google Colab)\n",
    "!pip install -q pandas numpy matplotlib scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import fisher_exact\n",
    "\n",
    "# Set display options\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "\n",
    "# Plotting style\n",
    "plt.style.use('seaborn-v0_8-darkgrid' if 'seaborn-v0_8-darkgrid' in plt.style.available else 'default')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3",
   "metadata": {},
   "source": [
    "## Load Processed Data\n",
    "\n",
    "Load the parsed turns and thread summary data generated by `run_analysis.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "turns_df = pd.read_csv('../data/processed/parsed_turns.csv')\n",
    "summary_df = pd.read_csv('../data/processed/thread_summary.csv')\n",
    "\n",
    "print(f\"Total conversation turns: {len(turns_df)}\")\n",
    "print(f\"Total threads: {len(summary_df)}\")\n",
    "print(f\"  Control: {len(summary_df[summary_df['condition'] == 'control'])}\")\n",
    "print(f\"  Contaminated: {len(summary_df[summary_df['condition'] == 'contaminated'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "## Response Classification Distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count response types by condition\n",
    "image_prompts = ['I1_KITCHEN', 'I2_BEDROOM', 'I3_ABSTRACT', 'I4_COFFEE']\n",
    "image_turns = turns_df[turns_df['prompt_id'].isin(image_prompts)]\n",
    "\n",
    "print(\"Response classifications for image prompts (I1-I4):\")\n",
    "print()\n",
    "for condition in ['control', 'contaminated']:\n",
    "    print(f\"{condition.capitalize()}:\")\n",
    "    cond_turns = image_turns[image_turns['condition'] == condition]\n",
    "    print(cond_turns['response_class'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "## Primary Analysis: Final Outcomes Per Thread\n",
    "\n",
    "For each thread and each prompt (I1-I4), we determine the final outcome:\n",
    "- If any retry succeeded → count as success\n",
    "- If all attempts failed (refusal or rate limit) → count as refusal\n",
    "\n",
    "This gives us one outcome per thread-prompt pair."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build final outcomes dataset\n",
    "final_outcomes = []\n",
    "\n",
    "for _, thread_row in summary_df.iterrows():\n",
    "    thread_id = thread_row['thread_id']\n",
    "    condition = thread_row['condition']\n",
    "    thread_turns = turns_df[turns_df['thread_id'] == thread_id]\n",
    "    \n",
    "    for prompt in image_prompts:\n",
    "        prompt_turns = thread_turns[thread_turns['prompt_id'] == prompt]\n",
    "        if len(prompt_turns) > 0:\n",
    "            # Check if any attempt succeeded\n",
    "            has_success = any(prompt_turns['response_class'] == 'image_success')\n",
    "            \n",
    "            if has_success:\n",
    "                final_class = 'image_success'\n",
    "            else:\n",
    "                # Failed - use last attempt's classification\n",
    "                final_class = prompt_turns.iloc[-1]['response_class']\n",
    "            \n",
    "            final_outcomes.append({\n",
    "                'thread_id': thread_id,\n",
    "                'condition': condition,\n",
    "                'prompt_id': prompt,\n",
    "                'final_outcome': final_class\n",
    "            })\n",
    "\n",
    "outcomes_df = pd.DataFrame(final_outcomes)\n",
    "print(f\"Total outcomes (one per thread-prompt): {len(outcomes_df)}\")\n",
    "print(f\"Expected: {len(summary_df)} threads × 4 prompts = {len(summary_df) * 4}\")\n",
    "outcomes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate statistics\n",
    "control_outcomes = outcomes_df[outcomes_df['condition'] == 'control']\n",
    "contaminated_outcomes = outcomes_df[outcomes_df['condition'] == 'contaminated']\n",
    "\n",
    "# Count refusals (policy_refusal, capability_refusal, or rate_limit that never succeeded)\n",
    "control_refusals = len(control_outcomes[\n",
    "    (control_outcomes['final_outcome'] == 'policy_refusal') |\n",
    "    (control_outcomes['final_outcome'] == 'capability_refusal') |\n",
    "    (control_outcomes['final_outcome'] == 'rate_limit')\n",
    "])\n",
    "contaminated_refusals = len(contaminated_outcomes[\n",
    "    (contaminated_outcomes['final_outcome'] == 'policy_refusal') |\n",
    "    (contaminated_outcomes['final_outcome'] == 'capability_refusal') |\n",
    "    (contaminated_outcomes['final_outcome'] == 'rate_limit')\n",
    "])\n",
    "\n",
    "control_success = len(control_outcomes[control_outcomes['final_outcome'] == 'image_success'])\n",
    "contaminated_success = len(contaminated_outcomes[contaminated_outcomes['final_outcome'] == 'image_success'])\n",
    "\n",
    "control_total = len(control_outcomes)\n",
    "contaminated_total = len(contaminated_outcomes)\n",
    "\n",
    "control_refusal_rate = control_refusals / control_total if control_total > 0 else 0\n",
    "contaminated_refusal_rate = contaminated_refusals / contaminated_total if contaminated_total > 0 else 0\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"FINAL OUTCOMES ANALYSIS\")\n",
    "print(\"=\"*70)\n",
    "print(f\"\\nControl:\")\n",
    "print(f\"  Total: {control_total}\")\n",
    "print(f\"  Success: {control_success}\")\n",
    "print(f\"  Refused: {control_refusals}\")\n",
    "print(f\"  Refusal Rate: {control_refusal_rate:.2%} ({control_refusals}/{control_total})\")\n",
    "\n",
    "print(f\"\\nContaminated:\")\n",
    "print(f\"  Total: {contaminated_total}\")\n",
    "print(f\"  Success: {contaminated_success}\")\n",
    "print(f\"  Refused: {contaminated_refusals}\")\n",
    "print(f\"  Refusal Rate: {contaminated_refusal_rate:.2%} ({contaminated_refusals}/{contaminated_total})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "## Statistical Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fisher's exact test\n",
    "contingency_table = [\n",
    "    [control_success, control_refusals],\n",
    "    [contaminated_success, contaminated_refusals]\n",
    "]\n",
    "\n",
    "odds_ratio, p_value = fisher_exact(contingency_table)\n",
    "\n",
    "print(\"Fisher's Exact Test:\")\n",
    "print(f\"  Contingency table: {contingency_table}\")\n",
    "print(f\"  Odds ratio: {odds_ratio}\")\n",
    "print(f\"  p-value: {p_value:.2e}\")\n",
    "\n",
    "# Cohen's h\n",
    "def cohen_h(p1, p2):\n",
    "    phi1 = 2 * np.arcsin(np.sqrt(p1))\n",
    "    phi2 = 2 * np.arcsin(np.sqrt(p2))\n",
    "    return abs(phi1 - phi2)\n",
    "\n",
    "h = cohen_h(contaminated_refusal_rate, control_refusal_rate)\n",
    "print(f\"\\nEffect Size (Cohen's h): {h:.2f}\")\n",
    "print(f\"  Interpretation: {'small' if abs(h) < 0.2 else 'medium' if abs(h) < 0.5 else 'large'} effect\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Visualization: Refusal Rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "\n",
    "conditions = ['Control', 'Contaminated']\n",
    "rates = [control_refusal_rate * 100, contaminated_refusal_rate * 100]\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "\n",
    "bars = ax.bar(conditions, rates, color=colors, alpha=0.8, edgecolor='black', linewidth=1.5)\n",
    "\n",
    "ax.set_ylabel('Refusal Rate (%)', fontsize=12, fontweight='bold')\n",
    "ax.set_xlabel('Condition', fontsize=12, fontweight='bold')\n",
    "ax.set_title('Image Generation Refusal Rates\\n(Final Outcomes)', fontsize=14, fontweight='bold', pad=20)\n",
    "ax.set_ylim(0, 105)\n",
    "\n",
    "# Add value labels\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height + 2,\n",
    "            f'{height:.2f}%',\n",
    "            ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "ax.set_axisbelow(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "## Per-Prompt Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze by prompt type\n",
    "print(\"Refusal rates by prompt type:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for prompt in image_prompts:\n",
    "    prompt_data = outcomes_df[outcomes_df['prompt_id'] == prompt]\n",
    "    \n",
    "    control_p = prompt_data[prompt_data['condition'] == 'control']\n",
    "    contam_p = prompt_data[prompt_data['condition'] == 'contaminated']\n",
    "    \n",
    "    control_refused = len(control_p[\n",
    "        (control_p['final_outcome'] == 'policy_refusal') |\n",
    "        (control_p['final_outcome'] == 'capability_refusal') |\n",
    "        (control_p['final_outcome'] == 'rate_limit')\n",
    "    ])\n",
    "    contam_refused = len(contam_p[\n",
    "        (contam_p['final_outcome'] == 'policy_refusal') |\n",
    "        (contam_p['final_outcome'] == 'capability_refusal') |\n",
    "        (contam_p['final_outcome'] == 'rate_limit')\n",
    "    ])\n",
    "    \n",
    "    prompt_name = prompt.split('_', 1)[1].title()\n",
    "    print(f\"\\n{prompt_name}:\")\n",
    "    print(f\"  Control refused: {control_refused}/{len(control_p)} ({control_refused/len(control_p)*100:.0f}%)\")\n",
    "    print(f\"  Contaminated refused: {contam_refused}/{len(contam_p)} ({contam_refused/len(contam_p)*100:.0f}%)\")\n",
    "    if contam_refused == len(contam_p):\n",
    "        print(f\"  ⚠️  100% refusal rate in contaminated condition!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16",
   "metadata": {},
   "source": [
    "## Breakthrough Analysis\n",
    "\n",
    "Identify which threads had successful image generation despite contamination:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find breakthrough cases\n",
    "breakthroughs = contaminated_outcomes[contaminated_outcomes['final_outcome'] == 'image_success']\n",
    "\n",
    "print(f\"Total breakthroughs: {len(breakthroughs)}\")\n",
    "print(\"\\nBreakthrough details:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for _, row in breakthroughs.iterrows():\n",
    "    thread_num = row['thread_id'].split('_')[1]\n",
    "    prompt_name = row['prompt_id'].split('_', 1)[1].title()\n",
    "    print(f\"Thread {thread_num}: {prompt_name} succeeded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "The analysis confirms an extreme violation state effect:\n",
    "\n",
    "### Key Findings\n",
    "\n",
    "1. **Massive Effect Size**: 96.67% refusal rate in contaminated vs. 0% in control\n",
    "2. **Statistical Certainty**: p < 0.0001 (Fisher's exact test), Cohen's h = 2.77\n",
    "3. **Coffee Cup Paradox**: 100% refusal despite zero semantic connection to copyright\n",
    "4. **Only 4 Breakthroughs**: Across 30 sessions (threads 12, 16, 25, 27)\n",
    "5. **Perfect Control**: 40/40 image requests succeeded without contamination\n",
    "\n",
    "### Implications\n",
    "\n",
    "This suggests ChatGPT's safety system maintains a persistent \"violation state\" after detecting a copyright violation. This state:\n",
    "- Persists across multiple conversation turns\n",
    "- Affects semantically unrelated requests (coffee cup)\n",
    "- Cannot be easily reset within the same conversation\n",
    "- Leads to systematic over-blocking of benign content\n",
    "\n",
    "The effect is robust, replicable, and represents a significant UX and safety system design challenge."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
